fp32
root@C.20616744:/home/assignment2-systems$ uv run cs336_systems/benchmarking_script.py --model_size small
Model: BasicsTransformerLM, Config: {'d_model': 768, 'd_ff': 3072, 'num_layers': 12, 'num_heads': 12}
    Forward pass time: 0.0308 +- 0.0002 seconds
    Backward pass time: 0.0521 +- 0.0003 seconds
root@C.20616744:/home/assignment2-systems$ uv run cs336_systems/benchmarking_script.py --model_size medium
Model: BasicsTransformerLM, Config: {'d_model': 1024, 'd_ff': 4096, 'num_layers': 24, 'num_heads': 16}
    Forward pass time: 0.0619 +- 0.0304 seconds
    Backward pass time: 0.0780 +- 0.0001 seconds
root@C.20616744:/home/assignment2-systems$ uv run cs336_systems/benchmarking_script.py --model_size large
Model: BasicsTransformerLM, Config: {'d_model': 1280, 'd_ff': 5120, 'num_layers': 36, 'num_heads': 20}
    Forward pass time: 0.0865 +- 0.0012 seconds
    Backward pass time: 0.1495 +- 0.0016 seconds
root@C.20616744:/home/assignment2-systems$ uv run cs336_systems/benchmarking_script.py --model_size xl
Model: BasicsTransformerLM, Config: {'d_model': 1600, 'd_ff': 6400, 'num_layers': 48, 'num_heads': 25}
    Forward pass time: 0.1187 +- 0.0012 seconds
    Backward pass time: 0.2627 +- 0.0011 seconds
root@C.20616744:/home/assignment2-systems$ uv run cs336_systems/benchmarking_script.py --model_size 2.7B
Model: BasicsTransformerLM, Config: {'d_model': 2560, 'd_ff': 10240, 'num_layers': 32, 'num_heads': 32}
    Forward pass time: 0.0921 +- 0.0001 seconds
    Backward pass time: 0.3886 +- 0.0002 seconds


fp16
root@C.20616744:/home/assignment2-systems$ uv run cs336_systems/benchmarking_script.py --model_size small --mixed_precision
Model: BasicsTransformerLM, Config: {'d_model': 768, 'd_ff': 3072, 'num_layers': 12, 'num_heads': 12}
    Forward pass time: 0.1026 +- 0.2083 seconds
    Backward pass time: 0.0573 +- 0.0009 seconds
root@C.20616744:/home/assignment2-systems$ uv run cs336_systems/benchmarking_script.py --model_size medium --mixed_precision
Model: BasicsTransformerLM, Config: {'d_model': 1024, 'd_ff': 4096, 'num_layers': 24, 'num_heads': 16}
    Forward pass time: 0.1536 +- 0.2325 seconds
    Backward pass time: 0.1117 +- 0.0042 seconds
root@C.20616744:/home/assignment2-systems$ uv run cs336_systems/benchmarking_script.py --model_size large --mixed_precision
Model: BasicsTransformerLM, Config: {'d_model': 1280, 'd_ff': 5120, 'num_layers': 36, 'num_heads': 20}
    Forward pass time: 0.1726 +- 0.2191 seconds
    Backward pass time: 0.1763 +- 0.0015 seconds
root@C.20616744:/home/assignment2-systems$ uv run cs336_systems/benchmarking_script.py --model_size xl --mixed_precision
Model: BasicsTransformerLM, Config: {'d_model': 1600, 'd_ff': 6400, 'num_layers': 48, 'num_heads': 25}
    Forward pass time: 0.2089 +- 0.2151 seconds
    Backward pass time: 0.2795 +- 0.0037 seconds
root@C.20616744:/home/assignment2-systems$ uv run cs336_systems/benchmarking_script.py --model_size 2.7B --mixed_precision
Model: BasicsTransformerLM, Config: {'d_model': 2560, 'd_ff': 10240, 'num_layers': 32, 'num_heads': 32}
    Forward pass time: 0.1608 +- 0.2131 seconds
    Backward pass time: 0.3063 +- 0.0032 seconds


bf16
root@C.20616744:/home/assignment2-systems$ uv run cs336_systems/benchmarking_script.py --model_size small --mixed_precision
Model: BasicsTransformerLM, Config: {'d_model': 768, 'd_ff': 3072, 'num_layers': 12, 'num_heads': 12}
    Forward pass time: 0.0479 +- 0.0517 seconds
    Backward pass time: 0.0516 +- 0.0084 seconds
root@C.20616744:/home/assignment2-systems$ uv run cs336_systems/benchmarking_script.py --model_size medium --mixed_precision
Model: BasicsTransformerLM, Config: {'d_model': 1024, 'd_ff': 4096, 'num_layers': 24, 'num_heads': 16}
    Forward pass time: 0.0923 +- 0.0583 seconds
    Backward pass time: 0.1100 +- 0.0036 seconds
root@C.20616744:/home/assignment2-systems$ uv run cs336_systems/benchmarking_script.py --model_size large --mixed_precision
Model: BasicsTransformerLM, Config: {'d_model': 1280, 'd_ff': 5120, 'num_layers': 36, 'num_heads': 20}
    Forward pass time: 0.1178 +- 0.0531 seconds
    Backward pass time: 0.1761 +- 0.0010 seconds
root@C.20616744:/home/assignment2-systems$ uv run cs336_systems/benchmarking_script.py --model_size xl --mixed_precision
Model: BasicsTransformerLM, Config: {'d_model': 1600, 'd_ff': 6400, 'num_layers': 48, 'num_heads': 25}
    Forward pass time: 0.1772 +- 0.1290 seconds
    Backward pass time: 0.2779 +- 0.0021 seconds
root@C.20616744:/home/assignment2-systems$ uv run cs336_systems/benchmarking_script.py --model_size 2.7B --mixed_precision
Model: BasicsTransformerLM, Config: {'d_model': 2560, 'd_ff': 10240, 'num_layers': 32, 'num_heads': 32}
    Forward pass time: 0.1077 +- 0.0582 seconds
    Backward pass time: 0.3007 +- 0.0086 seconds


Running: uv run nsys profile -o small_ctx_128 python cs336_systems/benchmarking_script.py --model_size small --context_length 128 --nsys
Collecting data...
Model: BasicsTransformerLM, Config: {'d_model': 768, 'd_ff': 3072, 'num_layers': 12, 'num_heads': 12}
    Forward pass time: 0.0377 +- 0.0004 seconds
    Backward pass time: 0.0698 +- 0.0003 seconds
Generating '/tmp/nsys-report-7b77.qdstrm'
[1/1] [========================100%] small_ctx_128.nsys-rep
Generated:
        /home/assignment2-systems/small_ctx_128.nsys-rep
Running: uv run nsys profile -o small_ctx_256 python cs336_systems/benchmarking_script.py --model_size small --context_length 256 --nsys
Collecting data...
Model: BasicsTransformerLM, Config: {'d_model': 768, 'd_ff': 3072, 'num_layers': 12, 'num_heads': 12}
    Forward pass time: 0.0389 +- 0.0005 seconds
    Backward pass time: 0.0736 +- 0.0007 seconds
Generating '/tmp/nsys-report-2060.qdstrm'
[1/1] [========================100%] small_ctx_256.nsys-rep
Generated:
        /home/assignment2-systems/small_ctx_256.nsys-rep
Running: uv run nsys profile -o small_ctx_512 python cs336_systems/benchmarking_script.py --model_size small --context_length 512 --nsys
Collecting data...
Model: BasicsTransformerLM, Config: {'d_model': 768, 'd_ff': 3072, 'num_layers': 12, 'num_heads': 12}
    Forward pass time: 0.0380 +- 0.0007 seconds
    Backward pass time: 0.0692 +- 0.0006 seconds
Generating '/tmp/nsys-report-f72b.qdstrm'
[1/1] [========================100%] small_ctx_512.nsys-rep
Generated:
        /home/assignment2-systems/small_ctx_512.nsys-rep
Running: uv run nsys profile -o small_ctx_1024 python cs336_systems/benchmarking_script.py --model_size small --context_length 1024 --nsys
Collecting data...
Model: BasicsTransformerLM, Config: {'d_model': 768, 'd_ff': 3072, 'num_layers': 12, 'num_heads': 12}
    Forward pass time: 0.0384 +- 0.0007 seconds
    Backward pass time: 0.0715 +- 0.0036 seconds
Generating '/tmp/nsys-report-99e1.qdstrm'
[1/1] [========================100%] small_ctx_1024.nsys-rep
Generated:
        /home/assignment2-systems/small_ctx_1024.nsys-rep

Running: uv run nsys profile -o medium_ctx_128 python cs336_systems/benchmarking_script.py --model_size medium --context_length 128 --nsys
Collecting data...
Model: BasicsTransformerLM, Config: {'d_model': 1024, 'd_ff': 4096, 'num_layers': 24, 'num_heads': 16}
    Forward pass time: 0.0876 +- 0.0314 seconds
    Backward pass time: 0.1422 +- 0.0015 seconds
Generating '/tmp/nsys-report-d9d7.qdstrm'
[1/1] [========================100%] medium_ctx_128.nsys-rep
Generated:
        /home/assignment2-systems/medium_ctx_128.nsys-rep
Running: uv run nsys profile -o medium_ctx_256 python cs336_systems/benchmarking_script.py --model_size medium --context_length 256 --nsys
Collecting data...
Model: BasicsTransformerLM, Config: {'d_model': 1024, 'd_ff': 4096, 'num_layers': 24, 'num_heads': 16}
    Forward pass time: 0.0889 +- 0.0365 seconds
    Backward pass time: 0.1420 +- 0.0025 seconds
Generating '/tmp/nsys-report-cf34.qdstrm'
[1/1] [========================100%] medium_ctx_256.nsys-rep
Generated:
        /home/assignment2-systems/medium_ctx_256.nsys-rep
Running: uv run nsys profile -o medium_ctx_512 python cs336_systems/benchmarking_script.py --model_size medium --context_length 512 --nsys
Collecting data...
Model: BasicsTransformerLM, Config: {'d_model': 1024, 'd_ff': 4096, 'num_layers': 24, 'num_heads': 16}
    Forward pass time: 0.0877 +- 0.0363 seconds
    Backward pass time: 0.1409 +- 0.0025 seconds
Generating '/tmp/nsys-report-a108.qdstrm'
[1/1] [========================100%] medium_ctx_512.nsys-rep
Generated:
        /home/assignment2-systems/medium_ctx_512.nsys-rep
Running: uv run nsys profile -o medium_ctx_1024 python cs336_systems/benchmarking_script.py --model_size medium --context_length 1024 --nsys
Collecting data...
Model: BasicsTransformerLM, Config: {'d_model': 1024, 'd_ff': 4096, 'num_layers': 24, 'num_heads': 16}
    Forward pass time: 0.0818 +- 0.0362 seconds
    Backward pass time: 0.1693 +- 0.0019 seconds
Generating '/tmp/nsys-report-3ab2.qdstrm'
[1/1] [========================100%] medium_ctx_1024.nsys-rep
Generated:
        /home/assignment2-systems/medium_ctx_1024.nsys-rep

Running: uv run nsys profile -o large_ctx_128 python cs336_systems/benchmarking_script.py --model_size large --context_length 128 --nsys
Collecting data...
Model: BasicsTransformerLM, Config: {'d_model': 1280, 'd_ff': 5120, 'num_layers': 36, 'num_heads': 20}
    Forward pass time: 0.1156 +- 0.0027 seconds
    Backward pass time: 0.2189 +- 0.0068 seconds
Generating '/tmp/nsys-report-4bdf.qdstrm'
[1/1] [========================100%] large_ctx_128.nsys-rep
Generated:
        /home/assignment2-systems/large_ctx_128.nsys-rep
Running: uv run nsys profile -o large_ctx_256 python cs336_systems/benchmarking_script.py --model_size large --context_length 256 --nsys
Collecting data...
Model: BasicsTransformerLM, Config: {'d_model': 1280, 'd_ff': 5120, 'num_layers': 36, 'num_heads': 20}
    Forward pass time: 0.1159 +- 0.0032 seconds
    Backward pass time: 0.2149 +- 0.0024 seconds
Generating '/tmp/nsys-report-f509.qdstrm'
[1/1] [========================100%] large_ctx_256.nsys-rep
Generated:
        /home/assignment2-systems/large_ctx_256.nsys-rep
Running: uv run nsys profile -o large_ctx_512 python cs336_systems/benchmarking_script.py --model_size large --context_length 512 --nsys
Collecting data...
Model: BasicsTransformerLM, Config: {'d_model': 1280, 'd_ff': 5120, 'num_layers': 36, 'num_heads': 20}
    Forward pass time: 0.1140 +- 0.0042 seconds
    Backward pass time: 0.2152 +- 0.0041 seconds
Generating '/tmp/nsys-report-d1f9.qdstrm'
[1/1] [========================100%] large_ctx_512.nsys-rep
Generated:
        /home/assignment2-systems/large_ctx_512.nsys-rep
Running: uv run nsys profile -o large_ctx_1024 python cs336_systems/benchmarking_script.py --model_size large --context_length 1024 --nsys
Collecting data...
Model: BasicsTransformerLM, Config: {'d_model': 1280, 'd_ff': 5120, 'num_layers': 36, 'num_heads': 20}
    Forward pass time: 0.1494 +- 0.0001 seconds
    Backward pass time: 0.3734 +- 0.0015 seconds
Generating '/tmp/nsys-report-21fa.qdstrm'
[1/1] [========================100%] large_ctx_1024.nsys-rep
Generated:
        /home/assignment2-systems/large_ctx_1024.nsys-rep

Running: uv run nsys profile -o xl_ctx_128 python cs336_systems/benchmarking_script.py --model_size xl --context_length 128 --nsys
Collecting data...
Model: BasicsTransformerLM, Config: {'d_model': 1600, 'd_ff': 6400, 'num_layers': 48, 'num_heads': 25}
    Forward pass time: 0.1561 +- 0.0028 seconds
    Backward pass time: 0.2926 +- 0.0032 seconds
Generating '/tmp/nsys-report-222b.qdstrm'
[1/1] [========================100%] xl_ctx_128.nsys-rep
Generated:
        /home/assignment2-systems/xl_ctx_128.nsys-rep
Running: uv run nsys profile -o xl_ctx_256 python cs336_systems/benchmarking_script.py --model_size xl --context_length 256 --nsys
Collecting data...
Model: BasicsTransformerLM, Config: {'d_model': 1600, 'd_ff': 6400, 'num_layers': 48, 'num_heads': 25}
    Forward pass time: 0.1723 +- 0.0371 seconds
    Backward pass time: 0.2979 +- 0.0032 seconds
Generating '/tmp/nsys-report-c54e.qdstrm'
[1/1] [========================100%] xl_ctx_256.nsys-rep
Generated:
        /home/assignment2-systems/xl_ctx_256.nsys-rep
Running: uv run nsys profile -o xl_ctx_512 python cs336_systems/benchmarking_script.py --model_size xl --context_length 512 --nsys
Collecting data...
Model: BasicsTransformerLM, Config: {'d_model': 1600, 'd_ff': 6400, 'num_layers': 48, 'num_heads': 25}
    Forward pass time: 0.1573 +- 0.0020 seconds
    Backward pass time: 0.4028 +- 0.0030 seconds
Generating '/tmp/nsys-report-810d.qdstrm'
[1/1] [========================100%] xl_ctx_512.nsys-rep
Generated:
        /home/assignment2-systems/xl_ctx_512.nsys-rep
Running: uv run nsys profile -o xl_ctx_1024 python cs336_systems/benchmarking_script.py --model_size xl --context_length 1024 --nsys
Collecting data...
Model: BasicsTransformerLM, Config: {'d_model': 1600, 'd_ff': 6400, 'num_layers': 48, 'num_heads': 25}
    Forward pass time: 0.2880 +- 0.0002 seconds
    Backward pass time: 0.6991 +- 0.0049 seconds
Generating '/tmp/nsys-report-d8b3.qdstrm'
[1/1] [========================100%] xl_ctx_1024.nsys-rep
Generated:
        /home/assignment2-systems/xl_ctx_1024.nsys-rep

unning: uv run nsys profile -o 2.7B_ctx_128 python cs336_systems/benchmarking_script.py --model_size 2.7B --context_length 128 --nsys
Collecting data...
Model: BasicsTransformerLM, Config: {'d_model': 2560, 'd_ff': 10240, 'num_layers': 32, 'num_heads': 32}
    Forward pass time: 0.1078 +- 0.0045 seconds
    Backward pass time: 0.3050 +- 0.0012 seconds
Generating '/tmp/nsys-report-607a.qdstrm'
[1/1] [========================100%] 2.7B_ctx_128.nsys-rep
Generated:
        /home/assignment2-systems/2.7B_ctx_128.nsys-rep
Running: uv run nsys profile -o 2.7B_ctx_256 python cs336_systems/benchmarking_script.py --model_size 2.7B --context_length 256 --nsys
Collecting data...
Model: BasicsTransformerLM, Config: {'d_model': 2560, 'd_ff': 10240, 'num_layers': 32, 'num_heads': 32}
    Forward pass time: 0.1066 +- 0.0050 seconds
    Backward pass time: 0.3918 +- 0.0012 seconds
Generating '/tmp/nsys-report-6d34.qdstrm'
[1/1] [========================100%] 2.7B_ctx_256.nsys-rep
Generated:
        /home/assignment2-systems/2.7B_ctx_256.nsys-rep
Running: uv run nsys profile -o 2.7B_ctx_512 python cs336_systems/benchmarking_script.py --model_size 2.7B --context_length 512 --nsys
Collecting data...
Model: BasicsTransformerLM, Config: {'d_model': 2560, 'd_ff': 10240, 'num_layers': 32, 'num_heads': 32}
    Forward pass time: 0.1858 +- 0.0002 seconds
    Backward pass time: 0.5771 +- 0.0015 seconds
Generating '/tmp/nsys-report-f85e.qdstrm'
[1/1] [========================100%] 2.7B_ctx_512.nsys-rep
Generated:
        /home/assignment2-systems/2.7B_ctx_512.nsys-rep
Running: uv run nsys profile -o 2.7B_ctx_1024 python cs336_systems/benchmarking_script.py --model_size 2.7B --context_length 1024 --nsys
Collecting data...
Model: BasicsTransformerLM, Config: {'d_model': 2560, 'd_ff': 10240, 'num_layers': 32, 'num_heads': 32}
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 192.06 MiB is free. Process 1598739 has 78.90 GiB memory in use. Of the allocated memory 76.69 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Generating '/tmp/nsys-report-86a7.qdstrm'
[1/1] [========================100%] 2.7B_ctx_1024.nsys-rep
Generated:
        /home/assignment2-systems/2.7B_ctx_1024.nsys-rep
Command failed: uv run nsys profile -o 2.7B_ctx_1024 python cs336_systems/benchmarking_script.py --model_size 2.7B --context_length 1024 --nsys
Error: Command '['uv', 'run', 'nsys', 'profile', '-o', '2.7B_ctx_1024', 'python', 'cs336_systems/benchmarking_script.py', '--model_size', '2.7B', '--context_length', '1024', '--nsys']' returned non-zero exit status 1.

Running: uv run cs336_systems/benchmarking_script.py --model_size 2.7B --context_length 128 --profile_memory 2.7B_ctx_128
Model: BasicsTransformerLM, Config: {'d_model': 2560, 'd_ff': 10240, 'num_layers': 32, 'num_heads': 32}
    Forward pass time: 0.0826 +- 0.0016 seconds
    Backward pass time: 0.3002 +- 0.0006 seconds
Running: uv run cs336_systems/benchmarking_script.py --model_size 2.7B --context_length 256 --profile_memory 2.7B_ctx_256
Model: BasicsTransformerLM, Config: {'d_model': 2560, 'd_ff': 10240, 'num_layers': 32, 'num_heads': 32}
    Forward pass time: 0.0920 +- 0.0002 seconds
    Backward pass time: 0.3892 +- 0.0003 seconds
Running: uv run cs336_systems/benchmarking_script.py --model_size 2.7B --context_length 512 --profile_memory 2.7B_ctx_512
Model: BasicsTransformerLM, Config: {'d_model': 2560, 'd_ff': 10240, 'num_layers': 32, 'num_heads': 32}
    Forward pass time: 0.1846 +- 0.0005 seconds
    Backward pass time: 0.5706 +- 0.0018 seconds

Running: uv run cs336_systems/benchmarking_script.py --model_size 2.7B --context_length 128 --mixed_precision --profile_memory 2.7B_ctx_128_mp
Model: BasicsTransformerLM, Config: {'d_model': 2560, 'd_ff': 10240, 'num_layers': 32, 'num_heads': 32}
    Forward pass time: 0.1542 +- 0.1729 seconds
    Backward pass time: 0.3058 +- 0.0057 seconds
Running: uv run cs336_systems/benchmarking_script.py --model_size 2.7B --context_length 256 --mixed_precision --profile_memory 2.7B_ctx_256_mp
Model: BasicsTransformerLM, Config: {'d_model': 2560, 'd_ff': 10240, 'num_layers': 32, 'num_heads': 32}
    Forward pass time: 0.1156 +- 0.0578 seconds
    Backward pass time: 0.3078 +- 0.0084 seconds
Running: uv run cs336_systems/benchmarking_script.py --model_size 2.7B --context_length 512 --mixed_precision --profile_memory 2.7B_ctx_512_mp
Model: BasicsTransformerLM, Config: {'d_model': 2560, 'd_ff': 10240, 'num_layers': 32, 'num_heads': 32}
    Forward pass time: 0.1002 +- 0.0396 seconds
    Backward pass time: 0.2929 +- 0.0047 seconds


Running: uv run cs336_systems/attention.py --impl naive
d_model: 16, sequence_length: 256
    Forward pass time: 0.0006 +- 0.0000 seconds
    Backward pass time: 0.0010 +- 0.0001 seconds
d_model: 16, sequence_length: 1024
    Forward pass time: 0.0006 +- 0.0000 seconds
    Backward pass time: 0.0012 +- 0.0000 seconds
d_model: 16, sequence_length: 4096
    Forward pass time: 0.0027 +- 0.0000 seconds
    Backward pass time: 0.0066 +- 0.0001 seconds
d_model: 16, sequence_length: 8192
    Forward pass time: 0.0102 +- 0.0000 seconds
    Backward pass time: 0.0245 +- 0.0001 seconds
d_model: 16, sequence_length: 16384
    Forward pass time: 0.0395 +- 0.0001 seconds
    Backward pass time: 0.0953 +- 0.0006 seconds
d_model: 32, sequence_length: 256
    Forward pass time: 0.0004 +- 0.0001 seconds
    Backward pass time: 0.0011 +- 0.0000 seconds
d_model: 32, sequence_length: 1024
    Forward pass time: 0.0006 +- 0.0000 seconds
    Backward pass time: 0.0011 +- 0.0000 seconds
d_model: 32, sequence_length: 4096
    Forward pass time: 0.0028 +- 0.0000 seconds
    Backward pass time: 0.0067 +- 0.0000 seconds
d_model: 32, sequence_length: 8192
    Forward pass time: 0.0105 +- 0.0000 seconds
    Backward pass time: 0.0248 +- 0.0001 seconds
d_model: 32, sequence_length: 16384
    Forward pass time: 0.0406 +- 0.0001 seconds
    Backward pass time: 0.0963 +- 0.0001 seconds
d_model: 64, sequence_length: 256
    Forward pass time: 0.0004 +- 0.0001 seconds
    Backward pass time: 0.0011 +- 0.0000 seconds
d_model: 64, sequence_length: 1024
    Forward pass time: 0.0006 +- 0.0000 seconds
    Backward pass time: 0.0011 +- 0.0000 seconds
d_model: 64, sequence_length: 4096
    Forward pass time: 0.0030 +- 0.0000 seconds
    Backward pass time: 0.0072 +- 0.0001 seconds
d_model: 64, sequence_length: 8192
    Forward pass time: 0.0117 +- 0.0001 seconds
    Backward pass time: 0.0272 +- 0.0001 seconds
d_model: 64, sequence_length: 16384
    Forward pass time: 0.0459 +- 0.0001 seconds
    Backward pass time: 0.1061 +- 0.0005 seconds
d_model: 128, sequence_length: 256
    Forward pass time: 0.0003 +- 0.0000 seconds
    Backward pass time: 0.0006 +- 0.0000 seconds
d_model: 128, sequence_length: 1024
    Forward pass time: 0.0004 +- 0.0000 seconds
    Backward pass time: 0.0009 +- 0.0000 seconds
d_model: 128, sequence_length: 4096
    Forward pass time: 0.0036 +- 0.0000 seconds
    Backward pass time: 0.0083 +- 0.0000 seconds
d_model: 128, sequence_length: 8192
    Forward pass time: 0.0140 +- 0.0000 seconds
    Backward pass time: 0.0314 +- 0.0001 seconds
d_model: 128, sequence_length: 16384
    Forward pass time: 0.0548 +- 0.0001 seconds
    Backward pass time: 0.1236 +- 0.0001 seconds

Running: uv run cs336_systems/attention.py --impl naive --causal
d_model: 16, sequence_length: 256
    Forward pass time: 0.0006 +- 0.0000 seconds
    Backward pass time: 0.0010 +- 0.0002 seconds
d_model: 16, sequence_length: 1024
    Forward pass time: 0.0006 +- 0.0000 seconds
    Backward pass time: 0.0012 +- 0.0000 seconds
d_model: 16, sequence_length: 4096
    Forward pass time: 0.0032 +- 0.0000 seconds
    Backward pass time: 0.0072 +- 0.0000 seconds
d_model: 16, sequence_length: 8192
    Forward pass time: 0.0123 +- 0.0000 seconds
    Backward pass time: 0.0268 +- 0.0000 seconds
d_model: 16, sequence_length: 16384
    Forward pass time: 0.0480 +- 0.0001 seconds
    Backward pass time: 0.1040 +- 0.0002 seconds
d_model: 32, sequence_length: 256
    Forward pass time: 0.0004 +- 0.0000 seconds
    Backward pass time: 0.0010 +- 0.0000 seconds
d_model: 32, sequence_length: 1024
    Forward pass time: 0.0004 +- 0.0000 seconds
    Backward pass time: 0.0010 +- 0.0000 seconds
d_model: 32, sequence_length: 4096
    Forward pass time: 0.0033 +- 0.0000 seconds
    Backward pass time: 0.0072 +- 0.0002 seconds
d_model: 32, sequence_length: 8192
    Forward pass time: 0.0126 +- 0.0000 seconds
    Backward pass time: 0.0269 +- 0.0001 seconds
d_model: 32, sequence_length: 16384
    Forward pass time: 0.0491 +- 0.0001 seconds
    Backward pass time: 0.1051 +- 0.0002 seconds
d_model: 64, sequence_length: 256
    Forward pass time: 0.0003 +- 0.0001 seconds
    Backward pass time: 0.0009 +- 0.0000 seconds
d_model: 64, sequence_length: 1024
    Forward pass time: 0.0004 +- 0.0000 seconds
    Backward pass time: 0.0010 +- 0.0000 seconds
d_model: 64, sequence_length: 4096
    Forward pass time: 0.0035 +- 0.0000 seconds
    Backward pass time: 0.0077 +- 0.0000 seconds
d_model: 64, sequence_length: 8192
    Forward pass time: 0.0138 +- 0.0000 seconds
    Backward pass time: 0.0292 +- 0.0000 seconds
d_model: 64, sequence_length: 16384
    Forward pass time: 0.0545 +- 0.0001 seconds
    Backward pass time: 0.1149 +- 0.0002 seconds
d_model: 128, sequence_length: 256
    Forward pass time: 0.0005 +- 0.0001 seconds
    Backward pass time: 0.0012 +- 0.0000 seconds
d_model: 128, sequence_length: 1024
    Forward pass time: 0.0006 +- 0.0000 seconds
    Backward pass time: 0.0012 +- 0.0000 seconds
d_model: 128, sequence_length: 4096
    Forward pass time: 0.0042 +- 0.0000 seconds
    Backward pass time: 0.0090 +- 0.0001 seconds
d_model: 128, sequence_length: 8192
    Forward pass time: 0.0162 +- 0.0001 seconds
    Backward pass time: 0.0340 +- 0.0001 seconds
d_model: 128, sequence_length: 16384
    Forward pass time: 0.0634 +- 0.0001 seconds
    Backward pass time: 0.1329 +- 0.0001 seconds

Running: uv run cs336_systems/attention.py --impl jit
d_model: 16, sequence_length: 256
    Forward pass time: 0.0019 +- 0.0002 seconds
    Backward pass time: 0.0011 +- 0.0002 seconds
d_model: 16, sequence_length: 1024
    Forward pass time: 0.0012 +- 0.0003 seconds
    Backward pass time: 0.0010 +- 0.0002 seconds
d_model: 16, sequence_length: 4096
    Forward pass time: 0.0043 +- 0.0217 seconds
    Backward pass time: 0.0032 +- 0.0001 seconds
d_model: 16, sequence_length: 8192
    Forward pass time: 0.0051 +- 0.0001 seconds
    Backward pass time: 0.0105 +- 0.0001 seconds
d_model: 16, sequence_length: 16384
    Forward pass time: 0.0181 +- 0.0001 seconds
    Backward pass time: 0.0405 +- 0.0001 seconds
d_model: 32, sequence_length: 256
    Forward pass time: 0.0013 +- 0.0001 seconds
    Backward pass time: 0.0009 +- 0.0002 seconds
d_model: 32, sequence_length: 1024
    Forward pass time: 0.0013 +- 0.0000 seconds
    Backward pass time: 0.0009 +- 0.0000 seconds
d_model: 32, sequence_length: 4096
    Forward pass time: 0.0023 +- 0.0000 seconds
    Backward pass time: 0.0034 +- 0.0000 seconds
d_model: 32, sequence_length: 8192
    Forward pass time: 0.0057 +- 0.0000 seconds
    Backward pass time: 0.0116 +- 0.0001 seconds
d_model: 32, sequence_length: 16384
    Forward pass time: 0.0202 +- 0.0002 seconds
    Backward pass time: 0.0426 +- 0.0002 seconds
d_model: 64, sequence_length: 256
    Forward pass time: 0.0020 +- 0.0003 seconds
    Backward pass time: 0.0017 +- 0.0003 seconds
d_model: 64, sequence_length: 1024
    Forward pass time: 0.0021 +- 0.0004 seconds
    Backward pass time: 0.0020 +- 0.0009 seconds
d_model: 64, sequence_length: 4096
    Forward pass time: 0.0032 +- 0.0002 seconds
    Backward pass time: 0.0044 +- 0.0004 seconds
d_model: 64, sequence_length: 8192
    Forward pass time: 0.0080 +- 0.0002 seconds
    Backward pass time: 0.0138 +- 0.0003 seconds
d_model: 64, sequence_length: 16384
    Forward pass time: 0.0254 +- 0.0006 seconds
    Backward pass time: 0.0522 +- 0.0005 seconds
d_model: 128, sequence_length: 256
    Forward pass time: 0.0012 +- 0.0001 seconds
    Backward pass time: 0.0009 +- 0.0001 seconds
d_model: 128, sequence_length: 1024
    Forward pass time: 0.0012 +- 0.0000 seconds
    Backward pass time: 0.0010 +- 0.0000 seconds
d_model: 128, sequence_length: 4096
    Forward pass time: 0.0033 +- 0.0000 seconds
    Backward pass time: 0.0050 +- 0.0000 seconds
d_model: 128, sequence_length: 8192
    Forward pass time: 0.0095 +- 0.0004 seconds
    Backward pass time: 0.0178 +- 0.0004 seconds
d_model: 128, sequence_length: 16384
    Forward pass time: 0.0336 +- 0.0002 seconds
    Backward pass time: 0.0694 +- 0.0003 seconds

Running: uv run cs336_systems/attention.py --impl jit --causal
d_model: 16, sequence_length: 256
    Forward pass time: 0.0020 +- 0.0002 seconds
    Backward pass time: 0.0009 +- 0.0001 seconds
d_model: 16, sequence_length: 1024
    Forward pass time: 0.0016 +- 0.0003 seconds
    Backward pass time: 0.0011 +- 0.0002 seconds
d_model: 16, sequence_length: 4096
    Forward pass time: 0.0022 +- 0.0001 seconds
    Backward pass time: 0.0032 +- 0.0001 seconds
d_model: 16, sequence_length: 8192
    Forward pass time: 0.0063 +- 0.0001 seconds
    Backward pass time: 0.0113 +- 0.0001 seconds
d_model: 16, sequence_length: 16384
    Forward pass time: 0.0214 +- 0.0003 seconds
    Backward pass time: 0.0443 +- 0.0002 seconds
d_model: 32, sequence_length: 256
    Forward pass time: 0.0025 +- 0.0008 seconds
    Backward pass time: 0.0019 +- 0.0004 seconds
d_model: 32, sequence_length: 1024
    Forward pass time: 0.0024 +- 0.0003 seconds
    Backward pass time: 0.0020 +- 0.0003 seconds
d_model: 32, sequence_length: 4096
    Forward pass time: 0.0033 +- 0.0008 seconds
    Backward pass time: 0.0039 +- 0.0003 seconds
d_model: 32, sequence_length: 8192
    Forward pass time: 0.0074 +- 0.0001 seconds
    Backward pass time: 0.0121 +- 0.0001 seconds
d_model: 32, sequence_length: 16384
    Forward pass time: 0.0230 +- 0.0001 seconds
    Backward pass time: 0.0453 +- 0.0003 seconds
d_model: 64, sequence_length: 256
    Forward pass time: 0.0010 +- 0.0000 seconds
    Backward pass time: 0.0008 +- 0.0005 seconds
d_model: 64, sequence_length: 1024
    Forward pass time: 0.0011 +- 0.0000 seconds
    Backward pass time: 0.0008 +- 0.0000 seconds
d_model: 64, sequence_length: 4096
    Forward pass time: 0.0027 +- 0.0000 seconds
    Backward pass time: 0.0039 +- 0.0000 seconds
d_model: 64, sequence_length: 8192
    Forward pass time: 0.0077 +- 0.0001 seconds
    Backward pass time: 0.0140 +- 0.0000 seconds
d_model: 64, sequence_length: 16384
    Forward pass time: 0.0284 +- 0.0000 seconds
    Backward pass time: 0.0552 +- 0.0001 seconds
d_model: 128, sequence_length: 256
    Forward pass time: 0.0011 +- 0.0001 seconds
    Backward pass time: 0.0009 +- 0.0003 seconds
d_model: 128, sequence_length: 1024
    Forward pass time: 0.0011 +- 0.0000 seconds
    Backward pass time: 0.0008 +- 0.0000 seconds
d_model: 128, sequence_length: 4096
    Forward pass time: 0.0034 +- 0.0001 seconds
    Backward pass time: 0.0052 +- 0.0001 seconds
d_model: 128, sequence_length: 8192
    Forward pass time: 0.0110 +- 0.0002 seconds
    Backward pass time: 0.0192 +- 0.0003 seconds
d_model: 128, sequence_length: 16384
    Forward pass time: 0.0389 +- 0.0002 seconds
    Backward pass time: 0.0738 +- 0.0003 seconds


Running: uv run cs336_systems/benchmarking_script.py --model_size 2.7B --context_length 128 --jit
Model: OptimizedModule, Config: {'d_model': 2560, 'd_ff': 10240, 'num_layers': 32, 'num_heads': 32}
/home/assignment2-systems/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:194: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
    Forward pass time: 0.0440 +- 0.0002 seconds
    Backward pass time: 0.2876 +- 0.0001 seconds
Running: uv run cs336_systems/benchmarking_script.py --model_size 2.7B --context_length 256 --jit
Model: OptimizedModule, Config: {'d_model': 2560, 'd_ff': 10240, 'num_layers': 32, 'num_heads': 32}
/home/assignment2-systems/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:194: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
    Forward pass time: 0.0825 +- 0.0009 seconds
    Backward pass time: 0.3689 +- 0.0003 seconds
Running: uv run cs336_systems/benchmarking_script.py --model_size 2.7B --context_length 512 --jit
Model: OptimizedModule, Config: {'d_model': 2560, 'd_ff': 10240, 'num_layers': 32, 'num_heads': 32}
/home/assignment2-systems/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:194: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
    Forward pass time: 0.1668 +- 0.0003 seconds
    Backward pass time: 0.5303 +- 0.0002 seconds


flash attention: forward
d = 16
   sequence_length    Triton     Torch
0            128.0  0.179200  0.590848
1            256.0  0.179200  0.595968
2            512.0  0.191488  0.621568
3           1024.0  0.197632  0.618496
4           2048.0  0.214016  0.623616
5           4096.0  0.295936  0.718848
6           8192.0  0.921600  2.530304
7          16384.0  3.581440  9.865216
flash attention: forward
d = 32
   sequence_length    Triton      Torch
0            128.0  0.196608   0.609280
1            256.0  0.193536   0.614400
2            512.0  0.194560   0.620544
3           1024.0  0.193536   0.621568
4           2048.0  0.211968   0.614400
5           4096.0  0.337920   0.737280
6           8192.0  0.987136   2.636800
7          16384.0  3.912704  10.277888
flash attention: forward
d = 64
   sequence_length    Triton      Torch
0            128.0  0.194560   0.605184
1            256.0  0.193536   0.613376
2            512.0  0.193536   0.622592
3           1024.0  0.197632   0.624640
4           2048.0  0.214016   0.619520
5           4096.0  0.449536   0.842752
6           8192.0  1.339392   3.070976
7          16384.0  5.258240  12.283392
flash attention: forward
d = 128
   sequence_length    Triton      Torch
0            128.0  0.198656   0.599040
1            256.0  0.194560   0.610304
2            512.0  0.194560   0.618496
3           1024.0  0.202752   0.618496
4           2048.0  0.245760   0.631808
5           4096.0  0.655360   1.081344
6           8192.0  2.193408   4.021248
7          16384.0  8.650752  16.121857
--
flash attention: backward
d = 16
   sequence_length     Triton      Torch
0            128.0   1.635840   1.290240
1            256.0   1.628160   1.337344
2            512.0   1.632256   1.338368
3           1024.0   1.655808   0.793600
4           2048.0   1.316864   0.770048
5           4096.0   1.318400   0.785408
6           8192.0   3.445760   2.826240
7          16384.0  13.613056  11.010560
flash attention: backward
d = 32
   sequence_length     Triton      Torch
0            128.0   1.318400   0.753152
1            256.0   1.307648   0.759808
2            512.0   1.323008   0.760832
3           1024.0   1.321984   0.769024
4           2048.0   1.305600   0.770048
5           4096.0   1.315840   0.786944
6           8192.0   3.662848   2.799616
7          16384.0  14.466048  11.580928
flash attention: backward
d = 64
   sequence_length     Triton      Torch
0            128.0   1.275904   0.752640
1            256.0   1.310720   0.764928
2            512.0   1.308672   0.771072
3           1024.0   1.323008   0.774144
4           2048.0   1.312768   0.782336
5           4096.0   1.303552   0.993280
6           8192.0   4.957184   3.894784
7          16384.0  19.295233  15.380480
flash attention: backward
d = 128
   sequence_length     Triton      Torch
0            128.0   1.081344   0.735232
1            256.0   1.318912   0.766464
2            512.0   1.312768   0.752640
3           1024.0   1.316864   0.769024
4           2048.0   1.317888   0.755712
5           4096.0   1.933312   1.519616
6           8192.0   7.303168   5.691904
7          16384.0  28.929024  23.190016

flash attention: forward
d = 16
   sequence_length    Triton     Torch
0            128.0  0.179200  0.591872
1            256.0  0.182272  0.605184
2            512.0  0.180224  0.609280
3           1024.0  0.200704  0.617472
4           2048.0  0.219136  0.611328
5           4096.0  0.225280  0.611328
6           8192.0  0.644096  1.459200
7          16384.0  2.489344  5.311488
flash attention: forward
d = 32
   sequence_length    Triton     Torch
0            128.0  0.201728  0.609280
1            256.0  0.199680  0.617472
2            512.0  0.198656  0.614400
3           1024.0  0.199680  0.622592
4           2048.0  0.218112  0.616448
5           4096.0  0.307200  0.613376
6           8192.0  0.822272  1.463296
7          16384.0  3.302400  5.398016
flash attention: forward
d = 64
   sequence_length    Triton     Torch
0            128.0  0.202752  0.612352
1            256.0  0.200704  0.619520
2            512.0  0.200704  0.612352
3           1024.0  0.200704  0.604160
4           2048.0  0.220160  0.610304
5           4096.0  0.344064  0.621568
6           8192.0  1.012736  1.508352
7          16384.0  3.995648  5.479424
flash attention: forward
d = 128
   sequence_length    Triton     Torch
0            128.0  0.199680  0.617472
1            256.0  0.201728  0.617472
2            512.0  0.196608  0.603136
3           1024.0  0.199680  0.606208
4           2048.0  0.217088  0.614400
5           4096.0  0.465920  0.631808
6           8192.0  1.620992  1.569792
7          16384.0  5.810688  5.737472
--
flash attention: backward
d = 16
   sequence_length    Triton     Torch
0            128.0  1.318400  0.790528
1            256.0  1.388544  0.806912
2            512.0  1.377280  0.792576
3           1024.0  1.376256  0.790528
4           2048.0  1.411584  0.827392
5           4096.0  1.403392  0.823296
6           8192.0  1.869824  1.315840
7          16384.0  7.211008  4.953088
flash attention: backward
d = 32
   sequence_length    Triton     Torch
0            128.0  1.319936  0.782336
1            256.0  1.376256  0.795648
2            512.0  1.380352  0.797184
3           1024.0  1.313792  0.811008
4           2048.0  1.331200  0.803840
5           4096.0  1.307136  0.783360
6           8192.0  1.878016  1.285120
7          16384.0  7.218176  4.937728
flash attention: backward
d = 64
   sequence_length    Triton     Torch
0            128.0  1.326080  0.785408
1            256.0  1.382400  0.795648
2            512.0  1.379328  0.789504
3           1024.0  1.175552  0.777216
4           2048.0  1.315840  0.779264
5           4096.0  1.377280  0.791552
6           8192.0  1.984512  1.335296
7          16384.0  7.464960  5.062144
flash attention: backward
d = 128
   sequence_length    Triton     Torch
0            128.0  1.371136  0.793600
1            256.0  1.331712  0.791552
2            512.0  1.142272  0.771072
3           1024.0  1.159168  0.775168
4           2048.0  1.313792  0.786944
5           4096.0  1.387520  0.811008
6           8192.0  2.117632  1.423360
7          16384.0  7.996416  5.329920


flash attention: forward
   sequence_length    Triton     Torch
0            128.0  0.018432  0.145408
1            256.0  0.021504  0.144384
2            512.0  0.022528  0.151552
3           1024.0  0.035840  0.154624
4           2048.0  0.055296  0.196608
5           4096.0  0.099328  0.699392
6           8192.0  0.290816  2.488320
7          16384.0  0.811008  9.859072
flash attention: forward
   sequence_length    Triton      Torch
0            128.0  0.016384   0.149504
1            256.0  0.018432   0.152576
2            512.0  0.025600   0.155648
3           1024.0  0.038912   0.157696
4           2048.0  0.066560   0.200704
5           4096.0  0.118784   0.714752
6           8192.0  0.389120   2.583552
7          16384.0  1.155072  10.188800
flash attention: forward
   sequence_length    Triton      Torch
0            128.0  0.018432   0.148480
1            256.0  0.024576   0.152576
2            512.0  0.033792   0.155648
3           1024.0  0.054272   0.154624
4           2048.0  0.093184   0.231424
5           4096.0  0.172032   0.818176
6           8192.0  0.645120   3.018752
7          16384.0  1.897984  12.179968
--
flash attention: forward
   sequence_length    Triton     Torch
0            128.0  0.051200  0.159744
1            256.0  0.015360  0.154624
2            512.0  0.019456  0.152576
3           1024.0  0.027648  0.154624
4           2048.0  0.044032  0.150528
5           4096.0  0.076800  0.412672
6           8192.0  0.206848  1.427456
7          16384.0  0.633856  5.171200
flash attention: forward
   sequence_length    Triton     Torch
0            128.0  0.014336  0.148480
1            256.0  0.017408  0.151552
2            512.0  0.022528  0.150528
3           1024.0  0.032768  0.152576
4           2048.0  0.054272  0.148480
5           4096.0  0.097280  0.412672
6           8192.0  0.287744  1.429504
7          16384.0  0.869376  5.197824
flash attention: forward
   sequence_length    Triton     Torch
0            128.0  0.017408  0.148480
1            256.0  0.021504  0.150528
2            512.0  0.028672  0.149504
3           1024.0  0.043008  0.147456
4           2048.0  0.073728  0.147456
5           4096.0  0.135168  0.420864
6           8192.0  0.425984  1.473536
7          16384.0  1.290240  5.287936

flash attention: forward
   sequence_length    Triton      Torch
0            128.0  0.293888   0.395264
1            256.0  0.156672   0.258048
2            512.0  0.191488   0.274432
3           1024.0  0.191488   0.286720
4           2048.0  0.219136   0.284672
5           4096.0  0.404480   0.749568
6           8192.0  0.856064   2.854912
7          16384.0  2.069504  10.993152
flash attention: forward
   sequence_length    Triton      Torch
0            128.0  0.492544   0.457728
1            256.0  0.351744   0.453632
2            512.0  0.427008   0.534528
3           1024.0  0.397312   0.535552
4           2048.0  0.424960   0.540672
5           4096.0  0.475136   0.760832
6           8192.0  1.132544   2.780160
7          16384.0  3.365888  11.521025
flash attention: forward
   sequence_length    Triton      Torch
0            128.0  0.217088   0.445440
1            256.0  0.320512   0.448512
2            512.0  0.319488   0.284672
3           1024.0  0.195584   0.449536
4           2048.0  0.358400   0.456704
5           4096.0  0.687104   0.988160
6           8192.0  1.943552   3.884032
7          16384.0  5.689856  15.323135
--
flash attention: forward
   sequence_length    Triton     Torch
0            128.0  0.624640  0.649216
1            256.0  0.235520  0.492544
2            512.0  0.431104  0.483328
3           1024.0  0.430080  0.483328
4           2048.0  0.456704  0.512000
5           4096.0  0.449536  0.491520
6           8192.0  0.951296  1.311744
7          16384.0  2.832896  4.939776
flash attention: forward
   sequence_length    Triton     Torch
0            128.0  0.421888  0.472064
1            256.0  0.428032  0.480256
2            512.0  0.430080  0.477696
3           1024.0  0.434176  0.493568
4           2048.0  0.451584  0.477184
5           4096.0  0.535552  0.490496
6           8192.0  1.213440  1.283072
7          16384.0  3.652608  4.920320
flash attention: forward
   sequence_length    Triton     Torch
0            128.0  0.248832  0.422912
1            256.0  0.424960  0.483328
2            512.0  0.432128  0.481280
3           1024.0  0.391680  0.487424
4           2048.0  0.465408  0.473088
5           4096.0  0.728064  0.663552
6           8192.0  1.787904  1.326080
7          16384.0  5.603328  5.039104